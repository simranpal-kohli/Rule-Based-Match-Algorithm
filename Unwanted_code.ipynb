{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import os\n",
    "from spacy.matcher import Matcher\n",
    "m_tool = Matcher(nlp.vocab)\n",
    "\n",
    "p1 = [{'LOWER': 'quickbrownfox'}]\n",
    "p2 = [{'LOWER': 'quick'}, {'IS_PUNCT': True}, {'LOWER': 'brown'}, {'IS_PUNCT': True}, {'LOWER': 'fox'}]\n",
    "p3 = [{'LOWER': 'quick'}, {'LOWER': 'brown'}, {'LOWER': 'fox'}]\n",
    "p4 =  [{'LOWER': 'quick'}, {'LOWER': 'brownfox'}]\n",
    "\n",
    "m_tool.add('QBF', None, p1, p2, p3, p4)\n",
    "\n",
    "sentence = nlp(u'The quick-brown-fox jumps over the lazy dog. The quick brown fox eats well. \\\n",
    "               the quickbrownfox is dead. the dog misses the quick brownfox')\n",
    "\n",
    "phrase_matches = m_tool(sentence)\n",
    "print(phrase_matches )\n",
    "\n",
    "\n",
    "for match_id, start, end in phrase_matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  \n",
    "    span = sentence[start:end]                   \n",
    "    print(match_id, string_id, start, end, span.text)\n",
    "    \n",
    "    \n",
    "Sentences = [\"This is the most beautiful place in the world.\",\n",
    "            \"This man has more skills to show in cricket than any other game.\",\n",
    "            \"Hi there! how was your ladakh trip last month?\",\n",
    "            \"Isn’t cricket supposed to be a team sport? I feel people should decide first whether cricket is a team game or an individual sport.\"]\n",
    "\n",
    "\n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def getSimilarityScore(raw_documents, words):\n",
    "    gen_docs = [[w.lower() for w in word_tokenize(text)] for text in raw_documents]\n",
    "    dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "    corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "    tf_idf = gensim.models.TfidfModel(corpus)\n",
    "    sims = gensim.similarities.Similarity('/usr/workdir',tf_idf[corpus],\n",
    "                                      num_features=len(dictionary))\n",
    "\n",
    "    query_doc_bow = dictionary.doc2bow(words)\n",
    "    query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "\n",
    "    return sims[query_doc_tf_idf]\n",
    "\n",
    "\n",
    "\n",
    "Sentences = [\"This is the most beautiful place in the world.\",\n",
    "            \"This man has more skills to show in cricket than any other game.\",\n",
    "            \"Hi there! how was your ladakh trip last month?\",\n",
    "            \"Isn’t cricket supposed to be a team sport? I feel people should decide first whether cricket is a team game or an individual sport.\"]\n",
    "\n",
    "words = [\"cricket\",\"sports\",\"team\",\"play\",\"match\"]\n",
    "words_lower = [w.lower() for w in words]\n",
    "getSimilarityScore(Sentences,words_lower)\n",
    "\n",
    "\n",
    "[[w.lower() for w in word_tokenize(text)] for text in Sentences]\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(Sentences[0])\n",
    "\n",
    "\n",
    "part_df=pd.DataFrame([{'Country Code': df1[(df1['code'] == code)].iloc[my_string_index]['code'], \n",
    "                                   'Unmatched Customer Name': df1[(df1['code'] == code)].iloc[my_string_index]['Names'],\n",
    "                                   'PCH Customer Name': df2[(df2['Country_code'] == code)].iloc[result]['Child Name'].tolist(),\n",
    "                                   'Nice ID':df2[(df2['Country_code'] == code)].iloc[result]['Child CRM NICE ID'].tolist()\n",
    "                                  }])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path='/Users/simranpal20gmail.com/Documents/Apple/Data/Week14/Manual_Match'\n",
    "# os.chdir(path)\n",
    "# unique_country_code_ls=actual_df.code.value_counts().index.tolist()\n",
    "# unique_country_code_ls\n",
    "# COLUMN_NAMES=['Country Code','Unmatched Full Cx Name','Unmatched Customer Name','PCH Customer Name']\n",
    "# w_write = pd.ExcelWriter('Match_match.xlsx')\n",
    "# for matched_no in range(1,5):\n",
    "#     country_main_df = pd.DataFrame(columns=COLUMN_NAMES)\n",
    "#     for cc in unique_country_code_ls:\n",
    "#         country_main_df=country_main_df.append(manual_match(cc,matched_no))\n",
    "#     country_main_df.to_excel(w_write, sheet_name='sheet_'  + str(matched_no))\n",
    "# w_write.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.pivot_table(exploded_main_df, index=['PCH Customer Name','Nice ID']).reset_index()\n",
    "\n",
    "# groupby_exploded_main_df=exploded_main_df.groupby(['Country Code', 'Unmatched Customer Name'],as_index=False)\n",
    "# groupby_exploded_main_df.size()\n",
    "# # groupby_exploded_main_df.groupby('ID', as_index=False).head(1)\n",
    "\n",
    "# df1[(df1['code'] == 'IT')].iloc[3]\n",
    "\n",
    "# indices_to_access = [155, 156, 531, 532]\n",
    "# accessed_mapping = map(my_list.__getitem__, indices_to_access)\n",
    "# accessed_list = list(accessed_mapping)\n",
    "# print(accessed_list)\n",
    "\n",
    "\n",
    "# df2_ls_colnames=['Child Name','Child CRM NICE ID']\n",
    "# df2[(df2['Country_code'] == 'IT')].iloc[indices_to_access][df2_ls_colnames]\n",
    "\n",
    "# df2[(df2['Country_code'] == 'IT')].iloc[indices_to_access]['Child Name']\n",
    "# df2[(df2['Country_code'] == 'IT')].iloc[indices_to_access]['Child CRM NICE ID']\n",
    "\n",
    "# [len(x1.intersection(x2)) >= num_matches for x1 in my_list_words_set for x2 in my_list_words_set]\n",
    "\n",
    "# path=\"\\\\Users\\\\simranpal20gmail.com\\\\Documents\\\\Apple\\\\Data\\\\Week14\\\\Manual_Match\\\\TARS_Processed\\\\\"\n",
    "# post_tars_file = glob.glob(path + \"*.xlsx\")\n",
    "# post_tars_df = pd.ExcelFile(post_tars_file)\n",
    "\n",
    "# unique_country_code_ls=actual_df.code.value_counts().index.tolist()\n",
    "# unique_country_code_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERAL_stop_words_ls=['solutions','solution','service','services','management','holding',\n",
    "                     'bank','technology','technologies','consulting','business','private','limited','software','hospital',\n",
    "                     'tech','industrial','corporation','company','europe','city','group','IT','region','international',\n",
    "                       'healthcare','community','east','west','north','marketing','global']\n",
    "AT_stopwords_ls=['it','ag','kg']\n",
    "BE_stopwords_ls=['belgium','bvba','stad','sa','de','la']\n",
    "CZ_stopwords_ls=['banka']\n",
    "DE_stopwords_ls=['germany','stadtwerke','stadt','krankenhaus','gmbh','ggmbh','sparkasse','deutschland','kliniken','berlin',\n",
    "                    'polizeipräsidium','gesellschaft','media','münchen','matt','klinikum','deutsche',\n",
    "                 'aktiengesellschaft','essen','pfalz','it','ag','kg','co.','co','für','st','des','der','und','nord',\n",
    "                'eg','am','von','se','ohg','die']\n",
    "DK_stopwords_ls=['danish','dansk','og','aps']\n",
    "ES_stopwords_ls=[]\n",
    "FI_stopwords_ls=['turun','oy']\n",
    "FR_stopwords_ls=['france','epargne','systemes','editions','paris','sa','et','gie','groupe','de','la','da','sas','caisse']\n",
    "GB_stopwords_ls=['uk','united', 'kingdom','u.k.','london','it','plc','the','national']\n",
    "IE_stopwords_ls=['ireland']\n",
    "IN_stopwords_ls=['india','limi','pvt','ltd','and','insurance']\n",
    "IT_stopwords_ls=['italia','spa','s.p.a.','srl','banca','logistica','data','computer','italian','group','ga','di']\n",
    "NL_stopwords_ls=['netherlands','nederland','solutions','europe','rotterdam','group','gemeente','b.v.','services','amsterdam',\n",
    "                 'omgevingsdienst','europe','stichting','groep','zorg','zorggroep','bv','van','en','de']\n",
    "NO_stopwords_ls=['norge','kommune','as','ggz','nv','avd']\n",
    "PL_stopwords_ls=['poland','polska','z.o.o.','s.a.','bank','media','sp','spółka']\n",
    "RU_stopwords_ls=[]\n",
    "SE_stopwords_ls=['sweden','ab','sverige','stockholm','kommun','nordic']\n",
    "TR_stopwords_ls=['pazarlama','yatirim','gida','menkul','san','a.ş','hi̇zmetleri̇','ve',]\n",
    "\n",
    "stop_word_dic={}\n",
    "stop_word_dic[\"General\"]=GENERAL_stop_words_ls\n",
    "stop_word_dic[\"AT\"]=AT_stopwords_ls\n",
    "stop_word_dic[\"BE\"]=BE_stopwords_ls\n",
    "# stop_word_dic[\"CH\"]=CH_stopwords_ls \n",
    "stop_word_dic[\"CZ\"]=CZ_stopwords_ls\n",
    "stop_word_dic[\"DE\"]=DE_stopwords_ls\n",
    "stop_word_dic[\"DK\"]=DK_stopwords_ls\n",
    "stop_word_dic[\"ES\"]=ES_stopwords_ls\n",
    "stop_word_dic[\"FI\"]=FI_stopwords_ls\n",
    "# stop_word_dic[\"FO\"]=FO_stopwords_ls\n",
    "stop_word_dic[\"FR\"]=FR_stopwords_ls\n",
    "stop_word_dic[\"GB\"]=GB_stopwords_ls\n",
    "stop_word_dic[\"IE\"]=IE_stopwords_ls\n",
    "stop_word_dic[\"IN\"]=IN_stopwords_ls\n",
    "# stop_word_dic[\"IS\"]=IS_stopwords_ls\n",
    "stop_word_dic[\"IT\"]=IT_stopwords_ls\n",
    "stop_word_dic[\"LU\"]=LU_stopwords_ls\n",
    "# stop_word_dic[\"MO\"]=MO_stopwords_ls\n",
    "stop_word_dic[\"NL\"]=NL_stopwords_ls\n",
    "stop_word_dic[\"NO\"]=NO_stopwords_ls\n",
    "stop_word_dic[\"PL\"]=PL_stopwords_ls\n",
    "# stop_word_dic[\"PT\"]=PT_stopwords_ls\n",
    "# stop_word_dic[\"RO\"]=RO_stopwords_ls\n",
    "# stop_word_dic[\"RS\"]=RS_stopwords_ls\n",
    "stop_word_dic[\"RU\"]=RU_stopwords_ls\n",
    "stop_word_dic[\"SE\"]=SE_stopwords_ls\n",
    "# stop_word_dic[\"SK\"]=SK_stopwords_ls\n",
    "# stop_word_dic[\"SM\"]=SM_stopwords_ls\n",
    "stop_word_dic[\"TR\"]=TR_stopwords_ls\n",
    "\n",
    "# FINAL_STOPWORDS_LS=GENERAL_stop_words_ls+AT_stopwords_ls+BE_stopwords_ls+CZ_stopwords_ls+DE_stopwords_ls+DK_stopwords_ls+FI_stopwords_ls+FR_stopwords_ls+GB_stopwords_ls+IE_stopwords_ls+IE_stopwords_ls+IN_stopwords_ls+IT_stopwords_ls+NL_stopwords_ls+NO_stopwords_ls+PL_stopwords_ls+SE_stopwords_ls+TR_stopwords_ls\n",
    "# df1['Names'] = df1['Names'].apply(lambda x: ' '.join([item for item in x.split() if item not in FINAL_STOPWORDS_LS]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
