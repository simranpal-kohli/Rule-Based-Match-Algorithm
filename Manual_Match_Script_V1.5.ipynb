{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                        Manual Match Automation Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing all the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import glob\n",
    "from nltk.corpus import stopwords\n",
    "from xlrd import XLRDError\n",
    "# nltk_stop_words = stopwords.words('english')\n",
    "from stop_words import get_stop_words\n",
    "from num2words import num2words \n",
    "\n",
    "from difflib import get_close_matches\n",
    "from re import sub\n",
    "# path='/Users/simranpal20gmail.com/Documents/Apple/Data/Week13/Manual_Match'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and making Transaction files ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Week_number=\"Week26\"\n",
    "admins_divide=3\n",
    "admins_divide_dict={1:'Ciara',2:'Neil',3:'Simran'}\n",
    "# admins_divide_dict={1:'Ciara',2:'Neil'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 73 partners Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/Users/simranpal20gmail.com/Documents/Apple/Data/\"+Week_number+\"/Manual_Match/73_Partners\"\n",
    "os.chdir(path)\n",
    "Partners_73_file=[f for f in os.listdir(path) if not f.startswith('.')]\n",
    "Partners_73_df = pd.read_excel(Partners_73_file[0],'STW Baseline - 73 Partners')\n",
    "Partners_73_ls=Partners_73_df['Apple HQ ID'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post TARS File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path='/Users/simranpal20gmail.com/Documents/Apple/Data/Week14/Manual_Match/Post_TARS'\n",
    "path=\"/Users/simranpal20gmail.com/Documents/Apple/Data/\"+Week_number+\"/Manual_Match/Post_TARS\"\n",
    "os.chdir(path)\n",
    "post_tars_file=[f for f in os.listdir(path) if not f.startswith('.')]\n",
    "post_tars_df = pd.read_excel(post_tars_file[0],'Transactions')\n",
    "post_tars_df=post_tars_df[post_tars_df['RESELLER HQ APPLE ID'].isin(Partners_73_ls)]\n",
    "post_tars_df['END CUSTOMER NAME']=post_tars_df['END CUSTOMER NAME'].astype(str)\n",
    "\n",
    "post_tars_df['name_ctry'] = post_tars_df[['END CUSTOMER NAME', 'END CUSTOMER COUNTRY']].apply(lambda x: ''.join(x), axis=1)\n",
    "status_ls=['For Review','Pre Parked']\n",
    "match_status_ls=['Exact Match']\n",
    "\n",
    "# status_ls=['Approved']\n",
    "# trans_pivot=pd.DataFrame({'ST_QTY':post_tars_df[post_tars_df['STATUS'].isin(status_ls)].groupby(['name_ctry'])['REPORTED QUANTITY'].sum()}).reset_index()\n",
    "# trans_pivot=pd.DataFrame({'ST_QTY':post_tars_df[post_tars_df['STATUS'].isin(status_ls)].groupby(['name_ctry','STATUS'])['REPORTED QUANTITY'].count()}).reset_index()\n",
    "# trans_pivot=post_tars_df[post_tars_df['STATUS'].isin(status_ls) & post_tars_df['END CUSTOMER NAME (MATCHED)'].isna()].groupby(['name_ctry','STATUS']).agg(Sum_Rpt_Qty=('REPORTED QUANTITY', 'sum'), Transaction_Counts=('name_ctry', 'count'))\n",
    "trans_pivot=post_tars_df[post_tars_df['STATUS'].isin(status_ls) & ~post_tars_df['MATCH STATUS'].isin(match_status_ls)].groupby(['name_ctry','STATUS','MATCH STATUS']).agg(Sum_Rpt_Qty=('REPORTED QUANTITY', 'sum'), Transaction_Counts=('name_ctry', 'count'))\n",
    "trans_pivot.reset_index(level=0, inplace=True)\n",
    "trans_pivot.reset_index(level=0, inplace=True)\n",
    "trans_pivot.reset_index(level=0, inplace=True)\n",
    "trans_pivot['code']=trans_pivot['name_ctry'].str[-2:]\n",
    "trans_pivot['Names']=trans_pivot['name_ctry'].str[:-2]\n",
    "\n",
    "df1=trans_pivot.copy()\n",
    "actual_df=df1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the Files for Auto_Match case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_ls=['For Review','Approved']\n",
    "match_status_ls=['Exact Match']\n",
    "\n",
    "trans_pivot_1=post_tars_df[post_tars_df['STATUS'].isin(status_ls) & post_tars_df['MATCH STATUS'].isin(match_status_ls)].groupby(['name_ctry','END CUSTOMER NAME','END CUSTOMER NAME (MATCHED)','END CUSTOMER COUNTRY',\n",
    "                                                                                                                               'END CUSTOMER COUNTRY (MATCHED)','STATUS']).agg(Sum_Rpt_Qty=('REPORTED QUANTITY', 'sum'), Transaction_Counts=('name_ctry', 'count'))\n",
    "trans_pivot_1.reset_index(level=0, inplace=True)\n",
    "trans_pivot_1.reset_index(level=0, inplace=True)\n",
    "trans_pivot_1.reset_index(level=0, inplace=True)\n",
    "trans_pivot_1.reset_index(level=0, inplace=True)\n",
    "trans_pivot_1.reset_index(level=0, inplace=True)\n",
    "trans_pivot_1.reset_index(level=0, inplace=True)\n",
    "\n",
    "trans_pivot_1_cols_ls = ['END CUSTOMER NAME', 'END CUSTOMER NAME (MATCHED)', 'END CUSTOMER COUNTRY',\n",
    "                'END CUSTOMER COUNTRY (MATCHED)','Transaction_Counts','Sum_Rpt_Qty','STATUS']\n",
    "# trans_pivot_1 = trans_pivot_1.reindex(columns=trans_pivot_1_cols_ls)\n",
    "trans_pivot_1 = trans_pivot_1[trans_pivot_1_cols_ls]\n",
    "\n",
    "admins_divide_dict1={1:'Ciara_auto',2:'Neil_auto',3:'Simran_auto'}\n",
    "# admins_divide_dict1={1:'Ciara_auto',2:'Simran_auto'}\n",
    "path=\"/Users/simranpal20gmail.com/Documents/Apple/Data/\"+Week_number+\"/Manual_Match\"\n",
    "os.chdir(path)\n",
    "Ciara_auto = pd.ExcelWriter('Auto_Match_Ciara.xlsx')\n",
    "Neil_auto = pd.ExcelWriter('Auto_Match_Neil.xlsx')\n",
    "Simran_auto = pd.ExcelWriter('Auto_Match_Simran.xlsx')\n",
    "\n",
    "for status in status_ls:\n",
    "    trans_pivot1_df=trans_pivot_1[trans_pivot_1['STATUS']==status]\n",
    "    admins_divide1_np=np.array_split(trans_pivot1_df, admins_divide)\n",
    "    for divide in range(0,admins_divide):\n",
    "        dynamic_var=admins_divide_dict1[divide+1]\n",
    "        admins_divide1_np[divide].to_excel(eval(dynamic_var), sheet_name=status,index=False)\n",
    "Ciara_auto.save()\n",
    "Neil_auto.save()\n",
    "Simran_auto.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unique End Customer Name Unmatched: 6172\n",
      "Total Transaction with Unmatched records: 12975\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Unique End Customer Name Unmatched:\",trans_pivot.shape[0])\n",
    "print(\"Total Transaction with Unmatched records:\",trans_pivot['Transaction_Counts'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCH File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path='/Users/simranpal20gmail.com/Documents/Apple/Data/Week14/Manual_Match/PCH'\n",
    "path=\"/Users/simranpal20gmail.com/Documents/Apple/Data/\"+Week_number+\"/Manual_Match/PCH\"\n",
    "os.chdir(path)\n",
    "try:\n",
    "    pch_file=[f for f in os.listdir(path) if not f.startswith('.')]\n",
    "    pch_df = pd.read_excel(pch_file[0])\n",
    "except XLRDError:\n",
    "    pch_df = pd.read_excel(pch_file[1])\n",
    "pch_df.columns = pch_df.iloc[0]\n",
    "pch_df = pch_df[1:]\n",
    "\n",
    "pch_columns_ls=['Country','Child CRM NICE ID','Child Name','Parent Name']\n",
    "pch_df=pch_df[pch_columns_ls]\n",
    "df2=pd.DataFrame({'count' : pch_df.groupby(['Country','Child CRM NICE ID','Child Name','Parent Name']).size()}).reset_index()\n",
    "# df2=pch_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Country Code File for BOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path='/Users/simranpal20gmail.com/Documents/Apple/Data/Week14/Manual_Match/Country_codes'\n",
    "path=\"/Users/simranpal20gmail.com/Documents/Apple/Data/\"+Week_number+\"/Manual_Match/Country_codes\"\n",
    "os.chdir(path)\n",
    "cc_write = pd.ExcelWriter('Simran_ManualMatch_ctry_codes.xlsx')\n",
    "\n",
    "# simran_code_ls=['IT','DK','IN','FR','AT','FI','RU','ES','CZ','PL','CH','TR','PT','RO','LU','LT']\n",
    "simran_code_ls=actual_df.code.value_counts().index.tolist()\n",
    "simran_code_cols=['name_ctry','Sum of Qty','Matching Status','Child Name','Parent Name','Nice ID']\n",
    "simran_code_df = pd.DataFrame(columns=simran_code_cols)\n",
    "\n",
    "\n",
    "for cc_code in simran_code_ls:\n",
    "    simran_code_df = pd.DataFrame(columns=simran_code_cols)\n",
    "    temp_code_df=trans_pivot[trans_pivot.code==cc_code][['name_ctry','Sum_Rpt_Qty','STATUS','Transaction_Counts']]\n",
    "    temp_code_df['Matching Status']=\"Not Matched\"\n",
    "    temp_code_df[\"Child Name\"] = \"\"\n",
    "    temp_code_df[\"Parent Name\"] = \"\"\n",
    "    temp_code_df[\"Nice ID\"] = \"\"\n",
    "#     simran_code_df=simran_code_df.append(temp_code_df)\n",
    "#     simran_code_df.to_excel(cc_write, sheet_name=str(cc_code))\n",
    "    temp_code_df.to_excel(cc_write, sheet_name=str(cc_code),index=False)\n",
    "cc_write.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/Users/simranpal20gmail.com/Documents/Apple/Data'\n",
    "os.chdir(path)\n",
    "\n",
    "xls = pd.ExcelFile('EMEIA_Country_Code.xlsx')\n",
    "emeia_codes = pd.read_excel(xls)\n",
    "\n",
    "# strip white space from the column\n",
    "df2['Country'] = df2['Country'].str.strip()\n",
    "emeia_codes['CountryName']=emeia_codes['CountryName'].str.strip()\n",
    "emeia_codes['ISO']=emeia_codes['ISO'].str.strip()\n",
    "df2['Country_code']=df2.merge(emeia_codes, how='left',left_on='Country', right_on='CountryName')['ISO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Country_wise_stop_words(df1_sw):\n",
    "    unique_country_code_ls=actual_df.code.value_counts().index.tolist()\n",
    "    for code in unique_country_code_ls:\n",
    "        if code in ['GB','IE','IN'] :\n",
    "            code_en=code\n",
    "            code='EN'\n",
    "        try:\n",
    "            stop_words_en = get_stop_words('en')\n",
    "            stop_words = get_stop_words(code.lower())\n",
    "            if code ==  'EN':\n",
    "                code=code_en\n",
    "            df1_sw.loc[df1_sw['code'] == code, 'Names'] = df1_sw[df1_sw.code==code]['Names'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop_words]))\n",
    "            df1_sw.loc[df1_sw['code'] == code, 'Names'] = df1_sw[df1_sw.code==code]['Names'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop_words_en]))\n",
    "        except:\n",
    "            continue\n",
    "    return df1_sw['Names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower case the end customer names in comparing file  \n",
    "df1['Names']=df1['Names'].str.lower()\n",
    "\n",
    "# Lower case the end customer names in PCH file\n",
    "df2['Child Name']=df2['Child Name'].str.lower()\n",
    "\n",
    "# removes numeric values from the df1 dataframe\n",
    "# df1['Names'] = df1['Names'].str.replace('\\d+', '')\n",
    "# df1['Names'] = df1['Names'].str.replace('\\b\\d+\\b *|\\b[a-z]\\b *', '')\n",
    "# df2['Child Name'] = df2['Child Name'].str.replace('\\d+', '')\n",
    "\n",
    "# removes the character with less than 2 characters in df1\n",
    "# df1['Names']=df1.Names.str.replace(r'\\b(\\w{1,1})\\b', '')\n",
    "\n",
    "# removes punctuations from df1\n",
    "df1[\"Names\"] = df1['Names'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# strip white space from the column\n",
    "df1['Names'] = df1['Names'].str.strip()\n",
    "\n",
    "df1['Names'] = Country_wise_stop_words(df1)\n",
    "df1_clean=df1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop_words for SHEET_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words_ls=[]\n",
    "# stop_word_dic_1={}\n",
    "# stop_word_dic_1[\"General\"]=['solutions','solution','service','services','management','holding',\n",
    "#                      'bank','technology','technologies','consulting','business','private','limited','software','hospital',\n",
    "#                      'tech','industrial','corporation','company','europe','city','group','IT','region','international',\n",
    "#                        'healthcare','community','east','west','north','marketing','global']\n",
    "# stop_word_dic_1[\"AT\"]=['austria','it','ag','kg','gmbh','ges','wien','österreich']\n",
    "# stop_word_dic_1[\"BE\"]=['belgium','bvba','stad','sa','de','la','nv','media']\n",
    "# stop_word_dic_1[\"BG\"]=[]\n",
    "# stop_word_dic_1[\"CH\"]=['sa','ag','schweiz','suisse','gmbh']\n",
    "# stop_word_dic_1[\"CZ\"]=['czech','banka','spol','česká','cz']\n",
    "# stop_word_dic_1[\"DE\"]=['germany','stadtwerke','stadt','krankenhaus','gmbh','ggmbh','sparkasse','deutschland','kliniken','berlin',\n",
    "#                     'polizeipräsidium','gesellschaft','media','münchen','matt','klinikum','deutsche',\n",
    "#                  'aktiengesellschaft','essen','pfalz','it','ag','kg','co.','co','für','st','des','der','und','nord',\n",
    "#                 'eg','am','von','se','ohg','die','bundes','dresden','internet','and','springer','bau','bundesministerium',\n",
    "#                     'nrw','db','gemeinnützige','kgaa','kreissparkasse','verlagsgesellschaft','mannheim','bosch','informatik',\n",
    "#                     'mbh','gmb','sachsen','gbr','stuttgart','rundfunk','digital','bayerische','stiftung','deutsches',\n",
    "#                     'ev','rhein','zweckverband','man','verlag','energie','national','axel','car','impact']\n",
    "# stop_word_dic_1[\"DK\"]=['denmark','danish','dansk','og','aps','danmark','kommune','as']\n",
    "# stop_word_dic_1[\"EG\"]=[]\n",
    "# stop_word_dic_1[\"ES\"]=[]\n",
    "# stop_word_dic_1[\"FI\"]=['finland','turun','oy','seudun','ab','uudenmaan','oyj','suomen']\n",
    "# stop_word_dic_1[\"FO\"]=[]\n",
    "# stop_word_dic_1[\"FR\"]=['france','epargne','systemes','editions','paris','sa','et','gie','groupe','de','la','da','sas','caisse',\n",
    "#                     'pour','it','snc','commune','francaise','des','se','ministere','sud','distribution','loisirs',\n",
    "#                     'centre','le','sarl']\n",
    "# stop_word_dic_1[\"GB\"]=['uk','united', 'kingdom','u.k.','london','it','plc','the','national','ltd','northern','authority','systems',\n",
    "#                     'emea','network','partners','fire','care','england','and','council','of','new','gb','nhs','trust','media',\n",
    "#                     'rescue','health','in','borough','llp','royal','on','communications','social','st','hospitals',\n",
    "#                     'office','for','ft','public','holdings','agency','worldwide','scotland','university','central',\n",
    "#                     'essex','design','defence','government','yorkshire','virgin','ambulance','ireland']\n",
    "# stop_word_dic_1[\"GR\"]=[]\n",
    "# stop_word_dic_1[\"IE\"]=['ireland','of']\n",
    "# stop_word_dic_1[\"IN\"]=['india','hindustan','limi','pvt','ltd','and','insurance','it','of','pr','systems','development','labs',\n",
    "#                     'life']\n",
    "# stop_word_dic_1[\"IS\"]=[]\n",
    "# stop_word_dic_1[\"IT\"]=['italy','italia','spa','s.p.a.','srl','banca','logistica','data','computer','italian','group','ga','di'\n",
    "#                     ,'italiane','servizi','il','de','media','informatica','societa','sa']\n",
    "# stop_word_dic_1[\"LT\"]=[]\n",
    "# stop_word_dic_1[\"LU\"]=['et','de']\n",
    "# stop_word_dic_1[\"MO\"]=[]\n",
    "# stop_word_dic_1[\"NL\"]=['netherlands','nederland','solutions','europe','rotterdam','group','gemeente','b.v.','services','amsterdam',\n",
    "#                  'omgevingsdienst','europe','stichting','groep','zorg','zorggroep','bv','van','en','de','ggz','nv','care','ict',\n",
    "#                     'centrum','utrecht','dienst','beheer','zuid','automotive']\n",
    "# stop_word_dic_1[\"NO\"]=['norway','norge','kommune','as','ggz','nv','avd','no','for','helse','politidistrikt','vest','republika',\n",
    "#                     'fylkeskommune','asa','sarpsborg']\n",
    "# stop_word_dic_1[\"PL\"]=['poland','polska','z.o.o.','s.a.','bank','media','sp','spółka']\n",
    "# stop_word_dic_1[\"PT\"]=['sa']\n",
    "# stop_word_dic_1[\"RO\"]=[]\n",
    "# stop_word_dic_1[\"RS\"]=[]\n",
    "# stop_word_dic_1[\"RU\"]=[]\n",
    "# stop_word_dic_1[\"SE\"]=['sweden','ab','sverige','stockholm','kommun','nordic','svenska','skåne']\n",
    "# stop_word_dic_1[\"SG\"]=[]\n",
    "# stop_word_dic_1[\"SK\"]=[]\n",
    "# stop_word_dic_1[\"SM\"]=[]\n",
    "# stop_word_dic_1[\"TR\"]=['pazarlama','yatirim','gida','menkul','san','a.ş','hi̇zmetleri̇','ve','opet','türk','kredi','yapi',\n",
    "#                     'tic','sağlik']\n",
    "# stop_word_dic_1[\"VA\"]=[]\n",
    "# stop_word_dic_1[\"SI\"]=[]\n",
    "# stop_word_dic_1[\"HU\"]=[]\n",
    "# stop_word_dic_1[\"HR\"]=[]\n",
    "# stop_word_dic_1[\"CY\"]=[]\n",
    "# stop_word_dic_1[\"MT\"]=[]\n",
    "# stop_word_dic_1[\"MC\"]=[]\n",
    "# stop_word_dic_1[\"CM\"]=[]\n",
    "# stop_word_dic_1[\"LI\"]=[]\n",
    "# stop_word_dic_1[\"TN\"]=[]\n",
    "# stop_word_dic_1[\"RE\"]=[]\n",
    "# stop_word_dic_1[\"GL\"]=[]\n",
    "# stop_word_dic_1[\"ZA\"]=[]\n",
    "# stop_word_dic_1[\"GE\"]=[]\n",
    "# stop_word_dic_1[\"ME\"]=[]\n",
    "# stop_word_dic_1[\"AR\"]=[]\n",
    "# stop_word_dic_1[\"EE\"]=[]\n",
    "# stop_word_dic_1[\"AD\"]=[]\n",
    "# stop_word_dic_1[\"PH\"]=[]\n",
    "# stop_word_dic_1[\"JE\"]=[]\n",
    "# stop_word_dic_1[\"BF\"]=[]\n",
    "# stop_word_dic_1[\"AE\"]=[]\n",
    "# stop_word_dic_1[\"LV\"]=[]\n",
    "# stop_word_dic_1[\"IM\"]=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_ls=[]\n",
    "stop_word_dic_1={}\n",
    "stop_word_dic_1[\"General\"]=[]\n",
    "stop_word_dic_1[\"AT\"]=['it','ag','kg','gmbh','ges','wien']\n",
    "stop_word_dic_1[\"BE\"]=['bvba','stad','sa','de','la','nv','media']\n",
    "stop_word_dic_1[\"BG\"]=[]\n",
    "stop_word_dic_1[\"CH\"]=['sa','ag','schweiz','suisse','gmbh']\n",
    "stop_word_dic_1[\"CZ\"]=['czech','banka','spol','česká','cz']\n",
    "stop_word_dic_1[\"DE\"]=['gmbh','ggmbh','ag','kg','co.','co','für','st','des','der','und','nord','eg','am','von','se','ohg','die','nrw','db','kgaa','mbh','gmb','sachsen','gbr']\n",
    "stop_word_dic_1[\"DK\"]=['denmark','danish','dansk','og','aps','danmark','kommune','as']\n",
    "stop_word_dic_1[\"EG\"]=[]\n",
    "stop_word_dic_1[\"ES\"]=[]\n",
    "stop_word_dic_1[\"FI\"]=['finland','turun','oy','seudun','ab','uudenmaan','oyj','suomen']\n",
    "stop_word_dic_1[\"FO\"]=[]\n",
    "stop_word_dic_1[\"FR\"]=['sa','et','gie','groupe','de','la','da','pour','it','snc','des','se','le','sarl']\n",
    "stop_word_dic_1[\"GB\"]=['it','plc','the','gb','nhs']\n",
    "stop_word_dic_1[\"GR\"]=[]\n",
    "stop_word_dic_1[\"IE\"]=[]\n",
    "stop_word_dic_1[\"IN\"]=['limi','pvt','ltd','and','it','of','pr','systems','development','labs','life']\n",
    "stop_word_dic_1[\"IS\"]=[]\n",
    "stop_word_dic_1[\"IT\"]=['srl','ga','di','il','de','sa']\n",
    "stop_word_dic_1[\"LT\"]=[]\n",
    "stop_word_dic_1[\"LU\"]=['et','de']\n",
    "stop_word_dic_1[\"MO\"]=[]\n",
    "stop_word_dic_1[\"NL\"]=['b.v.','stichting','groep','zorg','zorggroep','bv','van','en','de','ggz','nv','care','ict']\n",
    "stop_word_dic_1[\"NO\"]=['as','ggz','nv','avd','no','for','helse','asa']\n",
    "stop_word_dic_1[\"PL\"]=['z.o.o.','s.a.']\n",
    "stop_word_dic_1[\"PT\"]=['sa']\n",
    "stop_word_dic_1[\"RO\"]=[]\n",
    "stop_word_dic_1[\"RS\"]=[]\n",
    "stop_word_dic_1[\"RU\"]=[]\n",
    "stop_word_dic_1[\"SE\"]=['ab']\n",
    "stop_word_dic_1[\"SG\"]=[]\n",
    "stop_word_dic_1[\"SK\"]=[]\n",
    "stop_word_dic_1[\"SM\"]=[]\n",
    "stop_word_dic_1[\"TR\"]=['san','a.ş','ve','opet']\n",
    "stop_word_dic_1[\"VA\"]=[]\n",
    "stop_word_dic_1[\"SI\"]=[]\n",
    "stop_word_dic_1[\"HU\"]=[]\n",
    "stop_word_dic_1[\"HR\"]=[]\n",
    "stop_word_dic_1[\"CY\"]=[]\n",
    "stop_word_dic_1[\"MT\"]=[]\n",
    "stop_word_dic_1[\"MC\"]=[]\n",
    "stop_word_dic_1[\"CM\"]=[]\n",
    "stop_word_dic_1[\"LI\"]=[]\n",
    "stop_word_dic_1[\"TN\"]=[]\n",
    "stop_word_dic_1[\"RE\"]=[]\n",
    "stop_word_dic_1[\"GL\"]=[]\n",
    "stop_word_dic_1[\"ZA\"]=[]\n",
    "stop_word_dic_1[\"GE\"]=[]\n",
    "stop_word_dic_1[\"ME\"]=[]\n",
    "stop_word_dic_1[\"AR\"]=[]\n",
    "stop_word_dic_1[\"EE\"]=[]\n",
    "stop_word_dic_1[\"AD\"]=[]\n",
    "stop_word_dic_1[\"PH\"]=[]\n",
    "stop_word_dic_1[\"JE\"]=[]\n",
    "stop_word_dic_1[\"BF\"]=[]\n",
    "stop_word_dic_1[\"AE\"]=[]\n",
    "stop_word_dic_1[\"LV\"]=[]\n",
    "stop_word_dic_1[\"IM\"]=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding new codes into the dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IR', 'ML', 'QA', 'IL', 'GM', 'CW', 'PO']\n"
     ]
    }
   ],
   "source": [
    "unique_code_ls=actual_df.code.value_counts().index.tolist()\n",
    "existing_code_set=set(stop_word_dic_1.keys())\n",
    "new_code_ls = [x for x in unique_code_ls if x not in existing_code_set]\n",
    "print(new_code_ls)\n",
    "def add_new_keys_dict(stop_words_dic):\n",
    "    for new_code_idx,new_code in enumerate(new_code_ls):\n",
    "        print(new_code)\n",
    "        stop_words_dic.update({new_code:[]})\n",
    "    return stop_words_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR\n",
      "ML\n",
      "QA\n",
      "IL\n",
      "GM\n",
      "CW\n",
      "PO\n"
     ]
    }
   ],
   "source": [
    "stop_word_dic_1=add_new_keys_dict(stop_word_dic_1)\n",
    "stop_words_ls.append(stop_word_dic_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop_words for SHEET_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR\n",
      "ML\n",
      "QA\n",
      "IL\n",
      "GM\n",
      "CW\n",
      "PO\n",
      "IR\n",
      "ML\n",
      "QA\n",
      "IL\n",
      "GM\n",
      "CW\n",
      "PO\n"
     ]
    }
   ],
   "source": [
    "stop_word_dic_2={}\n",
    "stop_word_dic_2[\"General\"]=[]\n",
    "stop_word_dic_2[\"AT\"]=['gmbh','kg','gesmbh','ag']\n",
    "stop_word_dic_2[\"BE\"]=['sa','nv']\n",
    "stop_word_dic_2[\"BG\"]=[]\n",
    "stop_word_dic_2[\"CH\"]=['sa','gmbh','ag']\n",
    "stop_word_dic_2[\"CZ\"]=['czech']\n",
    "stop_word_dic_2[\"DE\"]=['gmbh','springer','kg','co','ag','it','holding','eg','mbh','gm','services']\n",
    "stop_word_dic_2[\"DK\"]=['aps']\n",
    "stop_word_dic_2[\"EG\"]=[]\n",
    "stop_word_dic_2[\"ES\"]=[]\n",
    "stop_word_dic_2[\"FI\"]=['oy','ab']\n",
    "stop_word_dic_2[\"FO\"]=[]\n",
    "stop_word_dic_2[\"FR\"]=['de','la','sas','caisse','louis']\n",
    "stop_word_dic_2[\"GB\"]=['uk','ltd','limited']\n",
    "stop_word_dic_2[\"GR\"]=[]\n",
    "stop_word_dic_2[\"HU\"]=['kft']\n",
    "stop_word_dic_2[\"IE\"]=['limited','ireland','ltd']\n",
    "stop_word_dic_2[\"IN\"]=['limited','pvt','ltd','private','insurance','company','lim','services']\n",
    "stop_word_dic_2[\"IS\"]=[]\n",
    "stop_word_dic_2[\"IT\"]=['srl','sp','legale','agricola']\n",
    "stop_word_dic_2[\"LT\"]=[]\n",
    "stop_word_dic_2[\"LU\"]=['de']\n",
    "stop_word_dic_2[\"MO\"]=[]\n",
    "stop_word_dic_2[\"NL\"]=['bv','oost','services','sales','service','se']\n",
    "stop_word_dic_2[\"NO\"]=['as','norge','og','iks','sogn']\n",
    "stop_word_dic_2[\"PL\"]=['sp']\n",
    "stop_word_dic_2[\"PT\"]=['şti']\n",
    "stop_word_dic_2[\"RO\"]=[]\n",
    "stop_word_dic_2[\"RS\"]=[]\n",
    "stop_word_dic_2[\"RU\"]=[]\n",
    "stop_word_dic_2[\"SE\"]=['ab','län']\n",
    "stop_word_dic_2[\"SG\"]=[]\n",
    "stop_word_dic_2[\"SK\"]=[]\n",
    "stop_word_dic_2[\"SM\"]=[]\n",
    "stop_word_dic_2[\"TR\"]=['kredi','şti','san']\n",
    "stop_word_dic_2[\"VA\"]=[]\n",
    "stop_word_dic_2[\"SI\"]=[]\n",
    "\n",
    "stop_word_dic_2[\"HR\"]=[]\n",
    "stop_word_dic_2[\"CY\"]=[]\n",
    "stop_word_dic_2[\"MT\"]=[]\n",
    "stop_word_dic_2[\"MC\"]=[]\n",
    "stop_word_dic_2[\"CM\"]=[]\n",
    "stop_word_dic_2[\"LI\"]=[]\n",
    "stop_word_dic_2[\"TN\"]=[]\n",
    "stop_word_dic_2[\"RE\"]=[]\n",
    "stop_word_dic_2[\"GL\"]=[]\n",
    "stop_word_dic_2[\"ZA\"]=[]\n",
    "stop_word_dic_2[\"GE\"]=[]\n",
    "stop_word_dic_2[\"ME\"]=[]\n",
    "stop_word_dic_2[\"AR\"]=[]\n",
    "stop_word_dic_2[\"EE\"]=[]\n",
    "stop_word_dic_2[\"AD\"]=[]\n",
    "stop_word_dic_2[\"PH\"]=[]\n",
    "stop_word_dic_2[\"JE\"]=[]\n",
    "stop_word_dic_2[\"BF\"]=[]\n",
    "stop_word_dic_2[\"AE\"]=[]\n",
    "stop_word_dic_2[\"LV\"]=[]\n",
    "stop_word_dic_2[\"IM\"]=[]\n",
    "stop_word_dic_2=add_new_keys_dict(stop_word_dic_2)\n",
    "\n",
    "stop_word_dic_2=add_new_keys_dict(stop_word_dic_2)\n",
    "stop_words_ls.append(stop_word_dic_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop_words for SHEET_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR\n",
      "ML\n",
      "QA\n",
      "IL\n",
      "GM\n",
      "CW\n",
      "PO\n",
      "IR\n",
      "ML\n",
      "QA\n",
      "IL\n",
      "GM\n",
      "CW\n",
      "PO\n"
     ]
    }
   ],
   "source": [
    "stop_word_dic_3={}\n",
    "stop_word_dic_3[\"General\"]=[]\n",
    "stop_word_dic_3[\"AT\"]=['kg','gmbh']\n",
    "stop_word_dic_3[\"BE\"]=[]\n",
    "stop_word_dic_3[\"BG\"]=[]\n",
    "stop_word_dic_3[\"CH\"]=[]\n",
    "stop_word_dic_3[\"CZ\"]=[]\n",
    "stop_word_dic_3[\"DE\"]=['gmbh','kg']\n",
    "stop_word_dic_3[\"DK\"]=[]\n",
    "stop_word_dic_3[\"EG\"]=[]\n",
    "stop_word_dic_3[\"ES\"]=[]\n",
    "stop_word_dic_3[\"FI\"]=[]\n",
    "stop_word_dic_3[\"FO\"]=[]\n",
    "stop_word_dic_3[\"FR\"]=[]\n",
    "stop_word_dic_3[\"GB\"]=['limited','ltd','trust']\n",
    "stop_word_dic_3[\"GR\"]=[]\n",
    "stop_word_dic_3[\"IE\"]=[]\n",
    "stop_word_dic_3[\"IN\"]=['pvt','ltd','private','limited']\n",
    "stop_word_dic_3[\"IS\"]=[]\n",
    "stop_word_dic_3[\"IT\"]=[]\n",
    "stop_word_dic_3[\"LT\"]=[]\n",
    "stop_word_dic_3[\"LU\"]=[]\n",
    "stop_word_dic_3[\"MO\"]=[]\n",
    "stop_word_dic_3[\"NL\"]=[]\n",
    "stop_word_dic_3[\"NO\"]=['møre','og']\n",
    "stop_word_dic_3[\"PL\"]=['sp']\n",
    "stop_word_dic_3[\"PT\"]=[]\n",
    "stop_word_dic_3[\"RO\"]=[]\n",
    "stop_word_dic_3[\"RS\"]=[]\n",
    "stop_word_dic_3[\"RU\"]=[]\n",
    "stop_word_dic_3[\"SE\"]=[]\n",
    "stop_word_dic_3[\"SG\"]=[]\n",
    "stop_word_dic_3[\"SK\"]=[]\n",
    "stop_word_dic_3[\"SM\"]=[]\n",
    "stop_word_dic_3[\"TR\"]=[]\n",
    "stop_word_dic_3[\"VA\"]=[]\n",
    "stop_word_dic_3[\"SI\"]=[]\n",
    "stop_word_dic_3[\"HU\"]=[]\n",
    "stop_word_dic_3[\"HR\"]=[]\n",
    "stop_word_dic_3[\"CY\"]=[]\n",
    "stop_word_dic_3[\"MT\"]=[]\n",
    "stop_word_dic_3[\"MC\"]=[]\n",
    "stop_word_dic_3[\"CM\"]=[]\n",
    "stop_word_dic_3[\"LI\"]=[]\n",
    "stop_word_dic_3[\"TN\"]=[]\n",
    "stop_word_dic_3[\"RE\"]=[]\n",
    "stop_word_dic_3[\"GL\"]=[]\n",
    "stop_word_dic_3[\"ZA\"]=[]\n",
    "stop_word_dic_3[\"GE\"]=[]\n",
    "stop_word_dic_3[\"ME\"]=[]\n",
    "stop_word_dic_3[\"AR\"]=[]\n",
    "stop_word_dic_3[\"EE\"]=[]\n",
    "stop_word_dic_3[\"AD\"]=[]\n",
    "stop_word_dic_3[\"PH\"]=[]\n",
    "stop_word_dic_3[\"JE\"]=[]\n",
    "stop_word_dic_3[\"BF\"]=[]\n",
    "stop_word_dic_3[\"AE\"]=[]\n",
    "stop_word_dic_3[\"LV\"]=[]\n",
    "stop_word_dic_3[\"IM\"]=[]\n",
    "stop_word_dic_3=add_new_keys_dict(stop_word_dic_3)\n",
    "\n",
    "stop_word_dic_3=add_new_keys_dict(stop_word_dic_3)\n",
    "stop_words_ls.append(stop_word_dic_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop_words for SHEET_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR\n",
      "ML\n",
      "QA\n",
      "IL\n",
      "GM\n",
      "CW\n",
      "PO\n",
      "IR\n",
      "ML\n",
      "QA\n",
      "IL\n",
      "GM\n",
      "CW\n",
      "PO\n"
     ]
    }
   ],
   "source": [
    "stop_word_dic_4={}\n",
    "stop_word_dic_4[\"General\"]=[]\n",
    "stop_word_dic_4[\"AT\"]=[]\n",
    "stop_word_dic_4[\"BE\"]=[]\n",
    "stop_word_dic_4[\"BG\"]=[]\n",
    "stop_word_dic_4[\"CH\"]=[]\n",
    "stop_word_dic_4[\"CZ\"]=[]\n",
    "stop_word_dic_4[\"DE\"]=['gmbh']\n",
    "stop_word_dic_4[\"DK\"]=[]\n",
    "stop_word_dic_4[\"EG\"]=[]\n",
    "stop_word_dic_4[\"ES\"]=[]\n",
    "stop_word_dic_4[\"FI\"]=[]\n",
    "stop_word_dic_4[\"FO\"]=[]\n",
    "stop_word_dic_4[\"FR\"]=[]\n",
    "stop_word_dic_4[\"GB\"]=[]\n",
    "stop_word_dic_4[\"GR\"]=[]\n",
    "stop_word_dic_4[\"IE\"]=[]\n",
    "stop_word_dic_4[\"IN\"]=['pvt','ltd']\n",
    "stop_word_dic_4[\"IS\"]=[]\n",
    "stop_word_dic_4[\"IT\"]=[]\n",
    "stop_word_dic_4[\"LT\"]=[]\n",
    "stop_word_dic_4[\"LU\"]=[]\n",
    "stop_word_dic_4[\"MO\"]=[]\n",
    "stop_word_dic_4[\"NL\"]=[]\n",
    "stop_word_dic_4[\"NO\"]=[]\n",
    "stop_word_dic_4[\"PL\"]=[]\n",
    "stop_word_dic_4[\"PT\"]=[]\n",
    "stop_word_dic_4[\"RO\"]=[]\n",
    "stop_word_dic_4[\"RS\"]=[]\n",
    "stop_word_dic_4[\"RU\"]=[]\n",
    "stop_word_dic_4[\"SE\"]=[]\n",
    "stop_word_dic_4[\"SG\"]=[]\n",
    "stop_word_dic_4[\"SK\"]=[]\n",
    "stop_word_dic_4[\"SM\"]=[]\n",
    "stop_word_dic_4[\"TR\"]=[]\n",
    "stop_word_dic_4[\"VA\"]=[]\n",
    "stop_word_dic_4[\"SI\"]=[]\n",
    "stop_word_dic_4[\"HU\"]=[]\n",
    "stop_word_dic_4[\"HR\"]=[]\n",
    "stop_word_dic_4[\"CY\"]=[]\n",
    "stop_word_dic_4[\"MT\"]=[]\n",
    "stop_word_dic_4[\"MC\"]=[]\n",
    "stop_word_dic_4[\"CM\"]=[]\n",
    "stop_word_dic_4[\"LI\"]=[]\n",
    "stop_word_dic_4[\"TN\"]=[]\n",
    "stop_word_dic_4[\"RE\"]=[]\n",
    "stop_word_dic_4[\"GL\"]=[]\n",
    "stop_word_dic_4[\"ZA\"]=[]\n",
    "stop_word_dic_4[\"GE\"]=[]\n",
    "stop_word_dic_4[\"ME\"]=[]\n",
    "stop_word_dic_4[\"AR\"]=[]\n",
    "stop_word_dic_4[\"EE\"]=[]\n",
    "stop_word_dic_4[\"AD\"]=[]\n",
    "stop_word_dic_4[\"PH\"]=[]\n",
    "stop_word_dic_4[\"JE\"]=[]\n",
    "stop_word_dic_4[\"BF\"]=[]\n",
    "stop_word_dic_4[\"AE\"]=[]\n",
    "stop_word_dic_4[\"LV\"]=[]\n",
    "stop_word_dic_4[\"IM\"]=[]\n",
    "stop_word_dic_4=add_new_keys_dict(stop_word_dic_4)\n",
    "\n",
    "stop_word_dic_4=add_new_keys_dict(stop_word_dic_4)\n",
    "stop_words_ls.append(stop_word_dic_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the list of dictionaries as an excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-641288ab1c61>:11: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  stop_words_df = pd.DataFrame({k:pd.Series(v[:min_length]) for k,v in stop_word_dic.items()})\n"
     ]
    }
   ],
   "source": [
    "# pd.DataFrame.from_dict(stop_word_dic)\n",
    "path=\"/Users/simranpal20gmail.com/Documents/Apple/Data/\"+Week_number+\"/Manual_Match/Corpus\"\n",
    "os.chdir(path)\n",
    "BuzzWord_write = pd.ExcelWriter('Buzz_Words.xlsx')\n",
    "for index,stop_word_dic in enumerate(stop_words_ls):\n",
    "    key_to_value_lengths = {k:len(v) for k, v in stop_word_dic.items()}\n",
    "    maximum = max(key_to_value_lengths, key=key_to_value_lengths.get)\n",
    "    # print(maximum, key_to_value_lengths[maximum])\n",
    "    min_length = key_to_value_lengths[maximum]\n",
    "    key_to_value_lengths = {k:len(v) for k, v in stop_word_dic.items()}\n",
    "    stop_words_df = pd.DataFrame({k:pd.Series(v[:min_length]) for k,v in stop_word_dic.items()})\n",
    "    stop_words_df.to_excel(BuzzWord_write, sheet_name=num2words(index+1),index=False)\n",
    "BuzzWord_write.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ini_string=\"3 sdc sd7f 87hbbsd 4545\"\n",
    "# # re.sub('[\\W_]+', '', ini_string)\n",
    "# # print(re.sub(r\"\\b\\d+\\b *|\\b[a-z]\\b *\",\"\",ini_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code=\"DE\"\n",
    "# FINAL_STOPWORDS_LS=stop_word_dic[\"General\"]+stop_word_dic[code]\n",
    "# # df1[df1.code==code]['Names'] = df1[df1.code==code]['Names'].apply(lambda x: ' '.join([item for item in x.split() if item not in FINAL_STOPWORDS_LS]))\n",
    "# df1.loc[df1['code'] == code, 'Names']=df1[df1.code==code]['Names'].apply(lambda x: ' '.join([item for item in x.split() if item not in FINAL_STOPWORDS_LS]))\n",
    "# df1[df1.code==code][['Names','code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s1=\"autoli cankor oto emn si\"\n",
    "# s2=\"ram si̇gorta a.ş.\"\n",
    "# set.intersection(set(s1.split(\" \")), set(s2.split(\" \")))\n",
    "\n",
    "# def get_words(input):\n",
    "#     return re.compile('\\w+').findall(input)\n",
    "\n",
    "# my_string=df1[df1.code==\"TR\"]['Names'].tolist()\n",
    "# my_string_words_set = [set(get_words(my_one_string)) for my_one_string in my_string]\n",
    "# print(my_string_words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# main_string=my_string_words_set[0]\n",
    "# x=\"eureko si̇gorta a.ş\"\n",
    "# x.split(\" \")\n",
    "# main_string.intersection(x)\n",
    "# set.intersection(set(s1.split(\" \")), set(s2.split(\" \")))\n",
    "\n",
    "# x=\"eureko si̇gorta a.ş\"\n",
    "# x=\"DOĞAN E DÖNÜŞÜM HİZ.\"\n",
    "# x=\"U F P (UK) Ltd\"\n",
    "# x=\"y-en-a.com sàrl\"\n",
    "\n",
    "\n",
    "# set(get_words(x))\n",
    "# set(x.split(\" \"))\n",
    "# re.sub(\"[^\\w' ]\", \"\", x).split()\n",
    "# [''.join(c for c in word if c.isalpha() or c==\"'\") for word in x.split()]\n",
    "\n",
    "# regex = r\"((?:[A-Za-z](\\.))*[A-Za-z]+)\\.?\"\n",
    "# subst = \"\\\\1\\\\2\"\n",
    "# result = list(re.sub(regex, subst, test_str, 0, re.MULTILINE).split())\n",
    "# print(result)\n",
    "# st1='E. M. ACCOUNTING S.R.L.'\n",
    "\n",
    "# re.compile('\\w+').findall(st1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusable Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode(df, lst_cols, fill_value='', preserve_index=False):\n",
    "    # make sure `lst_cols` is list-alike\n",
    "    if (lst_cols is not None\n",
    "        and len(lst_cols) > 0\n",
    "        and not isinstance(lst_cols, (list, tuple, np.ndarray, pd.Series))):\n",
    "        lst_cols = [lst_cols]\n",
    "    # all columns except `lst_cols`\n",
    "    idx_cols = df.columns.difference(lst_cols)\n",
    "    # calculate lengths of lists\n",
    "    lens = df[lst_cols[0]].str.len()\n",
    "    # preserve original index values    \n",
    "    idx = np.repeat(df.index.values, lens)\n",
    "    # create \"exploded\" DF\n",
    "    res = (pd.DataFrame({\n",
    "                col:np.repeat(df[col].values, lens)\n",
    "                for col in idx_cols},\n",
    "                index=idx)\n",
    "             .assign(**{col:np.concatenate(df.loc[lens>0, col].values)\n",
    "                            for col in lst_cols}))\n",
    "    # append those rows that have empty lists\n",
    "    if (lens == 0).any():\n",
    "        # at least one list in cells is empty\n",
    "        res = (res.append(df.loc[lens==0, idx_cols], sort=False)\n",
    "                  .fillna(fill_value))\n",
    "    # revert the original index order\n",
    "    res = res.sort_index()\n",
    "    # reset index if requested\n",
    "    if not preserve_index:        \n",
    "        res = res.reset_index(drop=True)\n",
    "    return res\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "def set_first_index_match(main_string,x):\n",
    "    try:\n",
    "        if(main_string[0]==x[0]):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return False   \n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "def get_words(input):\n",
    "#     regex = r\"((?:[A-Za-z](\\.))*[A-Za-z]+)\\.?\"\n",
    "#     subst = \"\\\\1\\\\2\"\n",
    "#     return list(re.sub(regex, subst, test_str, 0, re.MULTILINE).split())\n",
    "    return re.compile('\\w+').findall(input)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "# def all_match_main_list(tars_str,pch_str):\n",
    "#     tars_str_re = re.compile(tars_str)\n",
    "#     if tars_str_re.match(pch_str):\n",
    "#         return True\n",
    "#     else:\n",
    "#         return False\n",
    "    \n",
    "def all_match_main_list(string, pattern):\n",
    "    pattern = pattern.lower().split()\n",
    "    words = set(sub(r\"[^a-z0-9 ]\", \"\", string.lower()).split())  # Sanitize input\n",
    "    return all(get_close_matches(word, words, cutoff=0.8) for word in pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Logic Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Buzzwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_match_without_buzz(code,number_match,df1_clean,actual_df):\n",
    "    num_matches = number_match\n",
    "    exclude_index_ls=[]\n",
    "    FINAL_STOPWORDS_LS=stop_words_ls[num_matches-1][\"General\"]+stop_words_ls[num_matches-1][code]\n",
    "    df1=df1_clean.copy()\n",
    "    actual_df1=actual_df.copy()\n",
    "\n",
    "#     df1.loc[df1['code'] == code, 'Names'] = df1[df1.code==code]['Names'].apply(lambda x: ' '.join([item for item in x.split() if item not in FINAL_STOPWORDS_LS]))\n",
    "#     df1.loc[df1['code'] == code, 'Names'] = df1[df1.code==code]['Names'].apply(lambda x: ' '.join([item for item in x.split()]))\n",
    "    my_string=df1[df1.code==code]['Names'].tolist()\n",
    "    my_list=df2[df2.Country_code==code]['Child Name'].tolist()\n",
    "    COLUMN_NAMES=['Country Code','Unmatched Full Cx Name','Unmatched Customer Name','PCH Customer Name',\n",
    "                  'PCH Parent Name','Nice ID','STATUS','MATCH STATUS','Sum Rpt Qty','Transactions Count']\n",
    "    main_df = pd.DataFrame(columns=COLUMN_NAMES)\n",
    "\n",
    "#     my_string_words_set = [set(get_words(my_one_string)) for my_one_string in my_string]\n",
    "#     my_list_words_set = [set(get_words(x)) for x in my_list]\n",
    "    my_string_words_set = [get_words(my_one_string) for my_one_string in my_string]\n",
    "    my_list_words_set = [get_words(x) for x in my_list]\n",
    "    \n",
    "    for my_string_index,main_string in enumerate(my_string_words_set):\n",
    "        result = [i for i,x in enumerate(my_list_words_set) if len(set(main_string).intersection(set(x))) >= num_matches and set_first_index_match(main_string,x)]\n",
    "        if(num_matches==1):\n",
    "            match_with=2\n",
    "        else:\n",
    "            match_with=3\n",
    "        if len(result)>match_with:\n",
    "            changed_result_ls=[]\n",
    "            tars_str=my_string[my_string_index]\n",
    "            for result_index,result_index_val in enumerate(result):\n",
    "                pch_matched_child_name_str=df2[(df2['Country_code'] == code)].iloc[result_index_val]['Child Name']\n",
    "                pch_matched_child_name_str=pch_matched_child_name_str.replace(\"-\", \"\")\n",
    "                if all_match_main_list(pch_matched_child_name_str,tars_str):\n",
    "                    changed_result_ls.append(result_index_val)\n",
    "            result=changed_result_ls\n",
    "        if result:\n",
    "            accessed_mapping = map(my_list.__getitem__, result)\n",
    "            accessed_list = list(accessed_mapping)\n",
    "            part_df=pd.DataFrame([{'Country Code': df1[(df1['code'] == code)].iloc[my_string_index]['code'],\n",
    "                                   'Unmatched Full Cx Name': actual_df1[(actual_df1['code'] == code)].iloc[my_string_index]['Names'],\n",
    "                                   'Nice ID':df2[(df2['Country_code'] == code)].iloc[result]['Child CRM NICE ID'].tolist(),\n",
    "                                   'Unmatched Customer Name': df1[(df1['code'] == code)].iloc[my_string_index]['Names'],\n",
    "                                   'PCH Customer Name': df2[(df2['Country_code'] == code)].iloc[result]['Child Name'].tolist(),\n",
    "                                   'PCH Parent Name': df2[(df2['Country_code'] == code)].iloc[result]['Parent Name'].tolist(),\n",
    "                                   'STATUS': df1[(df1['code'] == code)].iloc[my_string_index]['STATUS'],\n",
    "                                   'MATCH STATUS': df1[(df1['code'] == code)].iloc[my_string_index]['MATCH STATUS'],\n",
    "                                   'Sum Rpt Qty': df1[(df1['code'] == code)].iloc[my_string_index]['Sum_Rpt_Qty'],\n",
    "                                   'Transactions Count': df1[(df1['code'] == code)].iloc[my_string_index]['Transaction_Counts']\n",
    "                                  }])\n",
    "            main_df=main_df.append(part_df)\n",
    "            \n",
    "            main_index=df1_clean[(df1_clean['code'] == code)].iloc[[my_string_index]].index\n",
    "            \n",
    "            exclude_index_ls.append(main_index.tolist()[0])\n",
    "    df1_clean=df1_clean[~df1_clean.index.isin(exclude_index_ls)].copy()\n",
    "    actual_df=actual_df[~actual_df.index.isin(exclude_index_ls)].copy()\n",
    "\n",
    "    if main_df.empty:\n",
    "        return (main_df,df1_clean,actual_df)\n",
    "    else:\n",
    "        exploded_main_df=explode(main_df, ['PCH Customer Name','PCH Parent Name','Nice ID'], fill_value='')\n",
    "        return (exploded_main_df,df1_clean,actual_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Buzzwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_match(code,number_match,df1_clean,actual_df):\n",
    "    num_matches = number_match\n",
    "    exclude_index_ls=[]\n",
    "    FINAL_STOPWORDS_LS=stop_words_ls[num_matches-1][\"General\"]+stop_words_ls[num_matches-1][code]\n",
    "    df1=df1_clean.copy()\n",
    "    actual_df1=actual_df.copy()\n",
    "\n",
    "    df1.loc[df1['code'] == code, 'Names'] = df1[df1.code==code]['Names'].apply(lambda x: ' '.join([item for item in x.split() if item not in FINAL_STOPWORDS_LS]))\n",
    "    my_string=df1[df1.code==code]['Names'].tolist()\n",
    "    my_list=df2[df2.Country_code==code]['Child Name'].tolist()\n",
    "    COLUMN_NAMES=['Country Code','Unmatched Full Cx Name','Nice ID','Unmatched Customer Name','PCH Customer Name',\n",
    "                  'PCH Parent Name','STATUS','MATCH STATUS','Sum Rpt Qty','Transactions Count']\n",
    "    main_df = pd.DataFrame(columns=COLUMN_NAMES)\n",
    "\n",
    "#     my_string_words_set = [set(get_words(my_one_string)) for my_one_string in my_string]\n",
    "#     my_list_words_set = [set(get_words(x)) for x in my_list]\n",
    "    my_string_words_set = [get_words(my_one_string) for my_one_string in my_string]\n",
    "    my_list_words_set = [get_words(x) for x in my_list]\n",
    "    \n",
    "    for my_string_index,main_string in enumerate(my_string_words_set):\n",
    "        result = [i for i,x in enumerate(my_list_words_set) if len(set(main_string).intersection(set(x))) >= num_matches and set_first_index_match(main_string,x)]\n",
    "        if len(result)>3:\n",
    "            changed_result_ls=[]\n",
    "            tars_str=my_string[my_string_index]\n",
    "            for result_index,result_index_val in enumerate(result):\n",
    "                pch_matched_child_name_str=df2[(df2['Country_code'] == code)].iloc[result_index_val]['Child Name']\n",
    "#                 pch_matched_child_name_str=pch_matched_child_name_str.replace(\"-\", \"\")\n",
    "                if all_match_main_list(pch_matched_child_name_str,tars_str):\n",
    "                    changed_result_ls.append(result_index_val)\n",
    "            result=changed_result_ls\n",
    "        if result:\n",
    "            accessed_mapping = map(my_list.__getitem__, result)\n",
    "            accessed_list = list(accessed_mapping)\n",
    "            part_df=pd.DataFrame([{'Country Code': df1[(df1['code'] == code)].iloc[my_string_index]['code'],\n",
    "                                   'Unmatched Full Cx Name': actual_df1[(actual_df1['code'] == code)].iloc[my_string_index]['Names'],\n",
    "                                   'Nice ID':df2[(df2['Country_code'] == code)].iloc[result]['Child CRM NICE ID'].tolist(),\n",
    "                                   'Unmatched Customer Name': df1[(df1['code'] == code)].iloc[my_string_index]['Names'],\n",
    "                                   'PCH Customer Name': df2[(df2['Country_code'] == code)].iloc[result]['Child Name'].tolist(),\n",
    "                                   'PCH Parent Name': df2[(df2['Country_code'] == code)].iloc[result]['Parent Name'].tolist(),\n",
    "                                   'STATUS': df1[(df1['code'] == code)].iloc[my_string_index]['STATUS'],\n",
    "                                   'MATCH STATUS': df1[(df1['code'] == code)].iloc[my_string_index]['MATCH STATUS'],\n",
    "                                   'Sum Rpt Qty': df1[(df1['code'] == code)].iloc[my_string_index]['Sum_Rpt_Qty'],\n",
    "                                   'Transactions Count': df1[(df1['code'] == code)].iloc[my_string_index]['Transaction_Counts']\n",
    "                                  }])\n",
    "            main_df=main_df.append(part_df)\n",
    "            \n",
    "            main_index=df1_clean[(df1_clean['code'] == code)].iloc[[my_string_index]].index\n",
    "            \n",
    "            exclude_index_ls.append(main_index.tolist()[0])\n",
    "    df1_clean=df1_clean[~df1_clean.index.isin(exclude_index_ls)].copy()\n",
    "    actual_df=actual_df[~actual_df.index.isin(exclude_index_ls)].copy()\n",
    "    main_df.insert(loc=2, column='Flag', value=0)\n",
    "#     main_df['Flag']=0\n",
    "    if main_df.empty:\n",
    "        return (main_df,df1_clean,actual_df)\n",
    "    else:\n",
    "        exploded_main_df=explode(main_df, ['PCH Customer Name','PCH Parent Name','Nice ID'], fill_value='')\n",
    "        return (exploded_main_df,df1_clean,actual_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', 3)\n",
    "# pd.set_option('display.width', 0)\n",
    "# pd.set_option('display.max_colwidth', 29)\n",
    "# exploded_df,df1_clean,actual_df=manual_match_without_buzz('DE',2,df1_clean,actual_df)\n",
    "# # exploded_df,df1_clean,actual_df=manual_match('IN',4,df1_clean,actual_df)\n",
    "# # print(manual_match('PT',4,df1_clean,actual_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', 3)\n",
    "# pd.set_option('display.width', 0)\n",
    "# pd.set_option('display.max_colwidth', 29)\n",
    "# print(df1_clean.shape)\n",
    "# print(actual_df.shape)\n",
    "# exploded_df,df1_clean,actual_df=manual_match('IN',3,df1_clean,actual_df)\n",
    "# print(df1_clean.shape)\n",
    "# print(actual_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df=pd.DataFrame({'code':['AT','IN','IN','IN','DE'],'col2':[0,1,2,3,4]})\n",
    "# print(test_df)\n",
    "# exclude_index_ls=[1,4]\n",
    "# code='IN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the final \"Appropriate Matched File\" in an excel format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path='/Users/simranpal20gmail.com/Documents/Apple/Data/Week14/Manual_Match'\n",
    "path=\"/Users/simranpal20gmail.com/Documents/Apple/Data/\"+Week_number+\"/Manual_Match\"\n",
    "os.chdir(path)\n",
    "unique_country_code_ls=actual_df.code.value_counts().index.tolist()\n",
    "unique_country_code_ls\n",
    "COLUMN_NAMES=['Country Code','Unmatched Full Cx Name','Flag','Nice ID','Unmatched Customer Name','PCH Customer Name',\n",
    "                  'PCH Parent Name','STATUS','MATCH STATUS','Sum Rpt Qty','Transactions Count']\n",
    "# df1_clean=df1.copy()\n",
    "Ciara = pd.ExcelWriter('Manual_Match_Ciara.xlsx')\n",
    "Neil = pd.ExcelWriter('Manual_Match_Neil.xlsx')\n",
    "Simran = pd.ExcelWriter('Manual_Match_Simran.xlsx')\n",
    "\n",
    "for matched_no in range(4, 0, -1):\n",
    "    country_main_df = pd.DataFrame(columns=COLUMN_NAMES)\n",
    "    for cc in unique_country_code_ls:\n",
    "        exploded_df,df1_clean,actual_df=manual_match(cc,matched_no,df1_clean,actual_df)\n",
    "        country_main_df=country_main_df.append(exploded_df)\n",
    "    admins_divide_np=np.array_split(country_main_df, admins_divide)\n",
    "    for divide in range(0,admins_divide):\n",
    "        dynamic_var=admins_divide_dict[divide+1]\n",
    "        admins_divide_np[divide].to_excel(eval(dynamic_var), sheet_name=num2words(matched_no),index=False)\n",
    "Ciara.save()\n",
    "Neil.save()\n",
    "Simran.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1=['media','impact','co','kg']\n",
    "# sorted(set(x1), key=x.index)\n",
    "\n",
    "# # x1={'media','impact','co','kg'}\n",
    "# # x2={'media','impact','gmbh','co.','kg'}\n",
    "# # for e in x2:\n",
    "# #     print(e)\n",
    "# # print(list(x2))\n",
    "# # set_first_index_match(x1,x2)\n",
    "\n",
    "# x1='alation india private limited'\n",
    "# x2='media impact gmbh co. kg'\n",
    "# get_words(x2)\n",
    "# # sorted(set(get_words(x2)), key=get_words(x2).index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------END HERE-----------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
